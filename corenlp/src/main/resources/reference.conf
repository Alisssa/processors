corenlp {

  language = English

  tokenize.language = en

  #pos.model = edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger
  #ner.model = edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz

  parser {

    genia {
      trainFile = /net/kate/storage/data/nlp/corpora/GENIA/stanford-basic-dependencies/train.conllx
      devFile = /net/kate/storage/data/nlp/corpora/GENIA/stanford-basic-dependencies/dev.conllx
      testFile = /net/kate/storage/data/nlp/corpora/GENIA/stanford-basic-dependencies/test.conllx
      # the trained model will be saved to this file
      model = en-bio-dep-genia-parser.model.txt.gz
    }

    wsj {
      trainFile = /net/kate/storage/data/nlp/corpora/ptb/stanford-basic-dependencies/train.conllx
      devFile = /net/kate/storage/data/nlp/corpora/ptb/stanford-basic-dependencies/dev.conllx
      testFile = /net/kate/storage/data/nlp/corpora/ptb/stanford-basic-dependencies/test.conllx
      # the trained model will be saved to this file
      model = en-bio-dep-wsj-parser.model.txt.gz
    }

    # CoreNLP doesn't expect the dimensions on the first line for some reason...
    w2v {
      embeddings = /net/kate/storage/data/nlp/corpora/word2vec/gigaword/vectors_first_line_removed.txt
      embeddingsDim = "200"
    }

    pmc {
      embeddings = /net/kate/storage/data/nlp/corpora/causal-assembly/pmc-openaccess-w2v.model
      embeddingsDim = "200"
    }

    w2v-pmc {
      embeddings = /net/kate/storage/data/nlp/corpora/word2vec/gigaword-pubmed/gigaword-pmc-vectors-first-line-removed.txt
      embeddingsDim = "200"
    }

  }

}
