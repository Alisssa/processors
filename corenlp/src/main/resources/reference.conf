corenlp {

  language = English

  tokenize.language = en

  #pos.model = edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger
  #ner.model = edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz

  parser {

    genia {
      # NOTE: the GENIA data should be combined with the WSJ data
      trainFile = /net/kate/storage/data/nlp/corpora/GENIA/stanford-basic-dependencies/train.conllx
      devFile = /net/kate/storage/data/nlp/corpora/GENIA/stanford-basic-dependencies/dev.conllx
      testFile = /net/kate/storage/data/nlp/corpora/GENIA/stanford-basic-dependencies/test.conllx
    }

    wsj {
      trainFile = /net/kate/storage/data/nlp/corpora/ptb/stanford-basic-dependencies/train.conllx
      devFile = /net/kate/storage/data/nlp/corpora/ptb/stanford-basic-dependencies/dev.conllx
      testFile = /net/kate/storage/data/nlp/corpora/ptb/stanford-basic-dependencies/test.conllx
    }

    # CoreNLP doesn't expect the dimensions on the first line for some reason...
    embeddings = /net/kate/storage/data/nlp/corpora/causal-assembly/pmc-openaccess-w2v.model
    # the length (number of features) of the embeddings
    embeddingsDim = "200"
    # the trained model will be saved to this file
    model = en-bio-dep-parser.model.txt.gz
  }

}
